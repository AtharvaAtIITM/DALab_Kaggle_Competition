{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This notebook is run on Kaggle. Therefore, the outputs in this notebook may not be properly rendered. Additionally, some elements in the code will be specific only to Kaggle (HF tokens, Kaggle initialisations, etc.). These may error out on other environments. It is recommended to run this notebook on an environment with CUDA installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-17T18:22:00.501669Z",
     "iopub.status.busy": "2025-11-17T18:22:00.501231Z",
     "iopub.status.idle": "2025-11-17T18:22:00.761867Z",
     "shell.execute_reply": "2025-11-17T18:22:00.761172Z",
     "shell.execute_reply.started": "2025-11-17T18:22:00.501645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/da5401-2025-data-challenge/test_data.json\n",
      "/kaggle/input/da5401-2025-data-challenge/sample_submission.csv\n",
      "/kaggle/input/da5401-2025-data-challenge/train_data.json\n",
      "/kaggle/input/da5401-2025-data-challenge/metric_names.json\n",
      "/kaggle/input/da5401-2025-data-challenge/metric_name_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:22:00.833523Z",
     "iopub.status.busy": "2025-11-17T18:22:00.833161Z",
     "iopub.status.idle": "2025-11-17T18:23:55.551097Z",
     "shell.execute_reply": "2025-11-17T18:23:55.550162Z",
     "shell.execute_reply.started": "2025-11-17T18:22:00.833503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -q sentence-transformers git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:23:55.552896Z",
     "iopub.status.busy": "2025-11-17T18:23:55.552668Z",
     "iopub.status.idle": "2025-11-17T18:23:55.708526Z",
     "shell.execute_reply": "2025-11-17T18:23:55.707921Z",
     "shell.execute_reply.started": "2025-11-17T18:23:55.552876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:23:55.709675Z",
     "iopub.status.busy": "2025-11-17T18:23:55.709436Z",
     "iopub.status.idle": "2025-11-17T18:23:56.271536Z",
     "shell.execute_reply": "2025-11-17T18:23:56.270705Z",
     "shell.execute_reply.started": "2025-11-17T18:23:55.709654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=secret_value_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:23:56.273372Z",
     "iopub.status.busy": "2025-11-17T18:23:56.273133Z",
     "iopub.status.idle": "2025-11-17T18:24:39.715841Z",
     "shell.execute_reply": "2025-11-17T18:24:39.715160Z",
     "shell.execute_reply.started": "2025-11-17T18:23:56.273354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 18:24:07.999434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763403848.249067      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763403848.314433      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030e62dddd18489e94b534bbda8647a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377cd9bbc6c64fc1b4b4c4012db94547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/997 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa7dd6a077446af9f14329188c84ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/18.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ed57926a8a4a8f816652d3a622a5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/58.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b4b7ed5d094881b0ec224d728004c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be10c1a050c4152b31e2cb96199a3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b981931f303847508126fb4b36014a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8758fdc75234acd8d1532b4cb025a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286f49f0452246c4a5b0bd51037f258b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974e1a1f09b84f87b0c52493be9962ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a862d586780a4ea088968b2c20c831b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31481612624a48da946597b9aaef5924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/312 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd21c88df774044af3e2af2d46e2029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ac8b8293644fb48dbf355372cb4c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/9.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1b966e6a1b49f085249c6c914be3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b5f19d06e54e47bfbdadf5e6f8a94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3_Dense/model.safetensors:   0%|          | 0.00/9.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 2048, 'do_lower_case': False, 'architecture': 'Gemma3TextModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 3072, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Dense({'in_features': 3072, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (4): Normalize()\n",
      ")\n",
      "Total number of parameters in the model: 307581696\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"google/embeddinggemma-300M\"\n",
    "model = SentenceTransformer(model_id).to(device=device)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(model)\n",
    "print(\"Total number of parameters in the model:\", sum([p.numel() for _, p in model.named_parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:39.717266Z",
     "iopub.status.busy": "2025-11-17T18:24:39.716579Z",
     "iopub.status.idle": "2025-11-17T18:24:40.333829Z",
     "shell.execute_reply": "2025-11-17T18:24:40.332401Z",
     "shell.execute_reply.started": "2025-11-17T18:24:39.717243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/da5401-2025-data-challenge/train_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/da5401-2025-data-challenge/test_data.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/da5401-2025-data-challenge/metric_names.json\", \"r\") as m:\n",
    "    met_names = json.load(m)\n",
    "\n",
    "met_emb = np.load(\"/kaggle/input/da5401-2025-data-challenge/metric_name_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:40.334989Z",
     "iopub.status.busy": "2025-11-17T18:24:40.334694Z",
     "iopub.status.idle": "2025-11-17T18:24:46.076736Z",
     "shell.execute_reply": "2025-11-17T18:24:46.075914Z",
     "shell.execute_reply.started": "2025-11-17T18:24:40.334939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric_name': 'transparency/attribution_to_sources',\n",
       " 'score': '9.0',\n",
       " 'user_prompt': 'मेरे धान की फसल की निचली पत्तियां पीली पड़ रही हैं और बाद में भूरी होकर सूख रही हैं। मुझे लगता है कि यह किसी पोषक तत्व की कमी है। आप मुझे बताएं कि आपने यह किस आधार पर बताया है और मैं इस बारे में और जानकारी कहाँ से प्राप्त कर सकता हूँ ताकि सही उपचार कर सकूँ?',\n",
       " 'response': 'धान की निचली पत्तियों का पीला पड़ना और फिर सूखना आमतौर पर नाइट्रोजन या फास्फोरस की कमी के लक्षण हो सकते हैं, हालांकि अन्य पोषक तत्वों की कमी या रोग भी इसका कारण हो सकते हैं। यह जानकारी भारतीय मृदा विज्ञान संस्थान (ICAR-Indian Institute of Soil Science), भोपाल और अखिल भारतीय समन्वित अनुसंधान परियोजना (AICRP) की मृदा विज्ञान पर प्रकाशित रिपोर्टों से ली गई है। मिट्टी की सटीक जांच और पोषक तत्व प्रबंधन के बारे में अधिक जानकारी आप ICAR-Indian Institute of Soil Science (iiss.icar.gov.in) की वेबसाइट और अपने क्षेत्र के कृषि विश्वविद्यालय के मृदा विज्ञान विभाग से प्राप्त कर सकते हैं। सटीक निदान के लिए अपनी मिट्टी और प्रभावित पौधों का प्रयोगशाला में परीक्षण करवाना अनिवार्य है।',\n",
       " 'system_prompt': 'You are a agriculture assistant. Understand and answer the question briefly.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:46.077781Z",
     "iopub.status.busy": "2025-11-17T18:24:46.077522Z",
     "iopub.status.idle": "2025-11-17T18:24:46.796826Z",
     "shell.execute_reply": "2025-11-17T18:24:46.795903Z",
     "shell.execute_reply.started": "2025-11-17T18:24:46.077761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_names = np.array(met_names)\n",
    "met_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:46.798718Z",
     "iopub.status.busy": "2025-11-17T18:24:46.798350Z",
     "iopub.status.idle": "2025-11-17T18:24:47.644451Z",
     "shell.execute_reply": "2025-11-17T18:24:47.643656Z",
     "shell.execute_reply.started": "2025-11-17T18:24:46.798699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping and trial embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:47.645481Z",
     "iopub.status.busy": "2025-11-17T18:24:47.645274Z",
     "iopub.status.idle": "2025-11-17T18:24:48.447290Z",
     "shell.execute_reply": "2025-11-17T18:24:48.446653Z",
     "shell.execute_reply.started": "2025-11-17T18:24:47.645464Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transparency/attribution_to_sources'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = data[1000][\"metric_name\"]\n",
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:48.449899Z",
     "iopub.status.busy": "2025-11-17T18:24:48.449567Z",
     "iopub.status.idle": "2025-11-17T18:24:48.462899Z",
     "shell.execute_reply": "2025-11-17T18:24:48.462176Z",
     "shell.execute_reply.started": "2025-11-17T18:24:48.449881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_met_emb = met_emb[np.argmax([met_names == met]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:48.464166Z",
     "iopub.status.busy": "2025-11-17T18:24:48.463897Z",
     "iopub.status.idle": "2025-11-17T18:24:49.163736Z",
     "shell.execute_reply": "2025-11-17T18:24:49.162874Z",
     "shell.execute_reply.started": "2025-11-17T18:24:48.464141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0788e3b9c764a7986bbbbfc5ded712c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = data[1000][\"user_prompt\"]\n",
    "prompt_emb = model.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:49.164699Z",
     "iopub.status.busy": "2025-11-17T18:24:49.164515Z",
     "iopub.status.idle": "2025-11-17T18:24:49.228287Z",
     "shell.execute_reply": "2025-11-17T18:24:49.227529Z",
     "shell.execute_reply.started": "2025-11-17T18:24:49.164685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08bfb4205af4a368099cd737ec39bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = data[1000][\"response\"]\n",
    "response_emb = model.encode(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:49.229189Z",
     "iopub.status.busy": "2025-11-17T18:24:49.228936Z",
     "iopub.status.idle": "2025-11-17T18:24:49.302845Z",
     "shell.execute_reply": "2025-11-17T18:24:49.302084Z",
     "shell.execute_reply.started": "2025-11-17T18:24:49.229172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "met_emb_idx = []\n",
    "prompts_lst = []\n",
    "sys_prompts_lst = []\n",
    "responses_lst = []\n",
    "scores_lst = []\n",
    "\n",
    "for d in data:\n",
    "    met_emb_idx += [met_emb[np.argmax([met_names == d[\"metric_name\"]]), :]]\n",
    "    prompts_lst += [d[\"user_prompt\"]]\n",
    "    sp = \"0\" if d[\"system_prompt\"] is None else d[\"system_prompt\"]\n",
    "    sys_prompts_lst += [sp]\n",
    "    responses_lst += [d[\"response\"]]\n",
    "    score = int(d[\"score\"].split(\".\")[0])\n",
    "    score += 1 if int(d[\"score\"].split(\".\")[1]) > 0 else 0\n",
    "    scores_lst += [score]\n",
    "\n",
    "test_met_emb_idx = []\n",
    "test_prompts_lst = []\n",
    "test_sys_prompts_lst = []\n",
    "test_responses_lst = []\n",
    "for t in test_data:\n",
    "    test_met_emb_idx += [met_emb[np.argmax([met_names == t[\"metric_name\"]]), :]]\n",
    "    test_prompts_lst += [t[\"user_prompt\"]]\n",
    "    test_sp = \"0\" if t[\"system_prompt\"] is None else t[\"system_prompt\"]\n",
    "    test_sys_prompts_lst += [test_sp]\n",
    "    test_responses_lst += [t[\"response\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Embeddings from Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:24:49.303860Z",
     "iopub.status.busy": "2025-11-17T18:24:49.303653Z",
     "iopub.status.idle": "2025-11-17T18:27:05.553118Z",
     "shell.execute_reply": "2025-11-17T18:27:05.552375Z",
     "shell.execute_reply.started": "2025-11-17T18:24:49.303844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3ad0f9fcf5452d8cab1b66fef116d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4822979de5ba485ba64b58924a6e04bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e331da72016942d79f43f2d6ca8b3f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix ready: (768, 4, 5000)\n",
      "Scores ready: (5000,)\n"
     ]
    }
   ],
   "source": [
    "met_emb_idx = np.array(met_emb_idx)\n",
    "\n",
    "# Batch encode prompts & responses\n",
    "prompt_emb = model.encode(prompts_lst, batch_size=20, show_progress_bar=True)      # (n_samples, dim)\n",
    "response_emb = model.encode(responses_lst, batch_size=20, show_progress_bar=True)  # (n_samples, dim)\n",
    "sys_prompt_emb = model.encode(sys_prompts_lst, batch_size=20, show_progress_bar=True)  # (n_samples, dim)\n",
    "\n",
    "# Final matrix: (dim, 4, n_samples)\n",
    "X = np.stack([met_emb_idx, sys_prompt_emb, prompt_emb, response_emb], axis=1)  # Shape: (n_samples, 4, dim)\n",
    "X = X.transpose(2, 1, 0)  # Shape: (dim, 4, n_samples)\n",
    "y = np.array(scores_lst)\n",
    "\n",
    "print(f\"Matrix ready: {X.shape}\")\n",
    "print(f\"Scores ready: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:27:05.554298Z",
     "iopub.status.busy": "2025-11-17T18:27:05.554003Z",
     "iopub.status.idle": "2025-11-17T18:28:44.957013Z",
     "shell.execute_reply": "2025-11-17T18:28:44.956317Z",
     "shell.execute_reply.started": "2025-11-17T18:27:05.554271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d872b0ce7a94a33845df89640fac178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b1e8bc2a5d42ebbac42fc00ada54a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e49ce55d66e44658b7b568f4e464a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix ready: (768, 4, 3638)\n"
     ]
    }
   ],
   "source": [
    "test_met_emb_idx = np.array(test_met_emb_idx)\n",
    "\n",
    "# Batch encode prompts & responses\n",
    "test_prompt_emb = model.encode(test_prompts_lst, batch_size=20, show_progress_bar=True)      # (n_samples, dim)\n",
    "test_response_emb = model.encode(test_responses_lst, batch_size=20, show_progress_bar=True)  # (n_samples, dim)\n",
    "test_sys_prompt_emb = model.encode(test_sys_prompts_lst, batch_size=20, show_progress_bar=True)  # (n_samples, dim)\n",
    "\n",
    "# Final matrix: (dim, 3, n_samples)\n",
    "X_test = np.stack([test_met_emb_idx, test_sys_prompt_emb, test_prompt_emb, test_response_emb], axis=1)  # Shape: (n_samples, 4, dim) -> transpose if needed\n",
    "X_test = X_test.transpose(2, 1, 0)  # Now (dim, 4, n_samples)\n",
    "\n",
    "print(f\"Matrix ready: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "Create a matrix where we combine 4 things:\n",
    "1. metric name embedding - extracted\n",
    "2. system prompt embedding - generated\n",
    "3. user prompt embedding - generated\n",
    "4. response embedding - generated\n",
    "\n",
    "the resultant matrix is of shape (embedding shape, 4 embeddings, n datapoints)\n",
    "\n",
    "Using the same resultant matrix flatten the data into 2 sets:\n",
    "\n",
    "1. embeddings shape * 3 embeddings (system prompt, user prompt and response), n datapoints\n",
    "2. embeddings shape * 1 embedding (metric name), n datapoints\n",
    "\n",
    "Apply self attention layers separately on both. Then apply cross attention across both (metric as KV for prompt-response query). Then have a few self attention layers and finally a pooled embedding output for metric-tuned distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:29:05.693624Z",
     "iopub.status.busy": "2025-11-17T18:29:05.692877Z",
     "iopub.status.idle": "2025-11-17T18:29:05.698236Z",
     "shell.execute_reply": "2025-11-17T18:29:05.697589Z",
     "shell.execute_reply.started": "2025-11-17T18:29:05.693600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 4, 5000), (5000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:29:06.750011Z",
     "iopub.status.busy": "2025-11-17T18:29:06.749221Z",
     "iopub.status.idle": "2025-11-17T18:29:06.754608Z",
     "shell.execute_reply": "2025-11-17T18:29:06.753793Z",
     "shell.execute_reply.started": "2025-11-17T18:29:06.749981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 4, 3638)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:29:08.583578Z",
     "iopub.status.busy": "2025-11-17T18:29:08.583024Z",
     "iopub.status.idle": "2025-11-17T18:29:08.670545Z",
     "shell.execute_reply": "2025-11-17T18:29:08.669572Z",
     "shell.execute_reply.started": "2025-11-17T18:29:08.583555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39/1173269738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "del new_model\n",
    "del optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # Used for classification problem formulation\n",
    "    def __init__(self, alpha=0.8, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "        \n",
    "class TripletHingeLoss(nn.Module):\n",
    "    # Used for metric learning formulation\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        dist_pos = (anchor - positive).pow(2).sum(1)\n",
    "        dist_neg = (anchor - negative).pow(2).sum(1)\n",
    "        losses = torch.relu(dist_pos - dist_neg + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    # Used for regression formulation\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta, reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = outputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # Give higher weight if target < 5\n",
    "        weights = torch.where(targets <= 5, 4.0, 1.0)\n",
    "\n",
    "        # per-sample huber loss\n",
    "        per_sample = self.huber(outputs, targets)\n",
    "\n",
    "        # weighted mean loss\n",
    "        loss = (per_sample * weights).sum() / weights.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:33:04.765348Z",
     "iopub.status.busy": "2025-11-17T18:33:04.764603Z",
     "iopub.status.idle": "2025-11-17T18:33:04.779136Z",
     "shell.execute_reply": "2025-11-17T18:33:04.778293Z",
     "shell.execute_reply.started": "2025-11-17T18:33:04.765319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_q1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.drop_attn = nn.Dropout(dropout)\n",
    "        \n",
    "        self.norm_ff = nn.LayerNorm(embed_dim)\n",
    "        ff_dim = embed_dim * 4\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.drop_ff = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        x = self.norm_q1(q)\n",
    "        attn, _ = self.mha(x, kv, kv)\n",
    "        q = q + self.drop_attn(attn)\n",
    "        \n",
    "        x = self.norm_ff(q)\n",
    "        ff = self.ffn(x)\n",
    "        q = q + self.drop_ff(ff)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, num_self_layers=3, num_cross_layers=2, num_final_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.self_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_self_layers)\n",
    "        self.final_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_final_layers)\n",
    "        \n",
    "        self.cross_blocks = nn.ModuleList([\n",
    "            CrossAttentionBlock(embed_dim, num_heads, dropout)\n",
    "            for _ in range(num_cross_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        m  = x[:, :1, :]\n",
    "        pr = x[:, 1:, :]\n",
    "\n",
    "        pr = self.self_encoder(pr)\n",
    "        m  = self.self_encoder(m)\n",
    "\n",
    "        combined = pr\n",
    "        for block in self.cross_blocks:\n",
    "            combined = block(combined, m)\n",
    "\n",
    "        combined = self.final_encoder(combined)\n",
    "        pooled = combined.mean(dim=1)\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:33:30.609078Z",
     "iopub.status.busy": "2025-11-17T18:33:30.608782Z",
     "iopub.status.idle": "2025-11-17T18:33:31.249301Z",
     "shell.execute_reply": "2025-11-17T18:33:31.248670Z",
     "shell.execute_reply.started": "2025-11-17T18:33:30.609054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionModel(\n",
       "  (self_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_blocks): ModuleList(\n",
       "    (0-7): 8 x CrossAttentionBlock(\n",
       "      (norm_q1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_attn): Dropout(p=0.4, inplace=False)\n",
       "      (norm_ff): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.4, inplace=False)\n",
       "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_ff): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = AttentionModel(embed_dim=X.shape[0], num_heads=16, \n",
    "                           num_self_layers=12, num_cross_layers=8, \n",
    "                           num_final_layers=4, dropout=0.4)\n",
    "new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:33:52.557157Z",
     "iopub.status.busy": "2025-11-17T18:33:52.556622Z",
     "iopub.status.idle": "2025-11-17T18:33:52.575396Z",
     "shell.execute_reply": "2025-11-17T18:33:52.574452Z",
     "shell.execute_reply.started": "2025-11-17T18:33:52.557132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print these over MSE/MAE for val; optimize on spearman for comp LB.\n",
    "def train(model: nn.Module, \n",
    "          X: torch.Tensor, \n",
    "          y: torch.Tensor, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          loss_fn: nn.Module, \n",
    "          scheduler: torch.optim.lr_scheduler._LRScheduler, \n",
    "          epochs: int = 100, \n",
    "          batch_size: int = 64,\n",
    "          val_split: float = 0.2,\n",
    "          device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "          clip_value: float = 1.0,\n",
    "          patience: int = 10,\n",
    "          seed: int = 42) -> dict:\n",
    "    torch.manual_seed(seed)\n",
    "    model.to(device)\n",
    "    \n",
    "    X = X.float().permute(2, 1, 0)\n",
    "    y = y.float()\n",
    "    \n",
    "    indices = torch.argsort(y)\n",
    "    low_idx = indices[:len(indices)//2]\n",
    "    high_idx = indices[len(indices)//2:]\n",
    "    \n",
    "    dataset = TensorDataset(X, y)\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "            emb = model(inputs)\n",
    "            size = len(emb) // 3\n",
    "            anchor = emb[:size]\n",
    "            positive = emb[size:2*size]\n",
    "            negative = emb[2*size:3*size]\n",
    "            if size == 0: continue\n",
    "            loss = loss_fn(anchor, positive, negative)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        if not train_losses: continue  # No valid batches\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f}\")\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_loss = evaluate(model, val_loader, loss_fn, device)['loss']\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "    return history\n",
    "\n",
    "def evaluate(model: nn.Module, \n",
    "             loader: DataLoader, \n",
    "             loss_fn: nn.Module, \n",
    "             device: torch.device) -> dict:\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            targets = batch[1].to(device)\n",
    "            emb = model(inputs)\n",
    "            size = len(emb) // 3\n",
    "            anchor = emb[:size]\n",
    "            positive = emb[size:2*size]\n",
    "            negative = emb[2*size:3*size]\n",
    "            if size == 0: continue\n",
    "            loss = loss_fn(anchor, positive, negative)\n",
    "            all_losses.append(loss.item())\n",
    "    result = {'loss': sum(all_losses) / len(all_losses) if all_losses else float('inf')}\n",
    "    return result\n",
    "\n",
    "def test(model: nn.Module, \n",
    "         X_test: torch.Tensor, \n",
    "         batch_size: int = 128,\n",
    "         device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')) -> torch.Tensor:\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    X_test = X_test.float().permute(2, 1, 0)\n",
    "    test_ds = TensorDataset(X_test)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    model.eval()\n",
    "    all_emb = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            emb = model(inputs)\n",
    "            all_emb.append(emb.cpu())\n",
    "    return torch.cat(all_emb, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:33:57.177574Z",
     "iopub.status.busy": "2025-11-17T18:33:57.177112Z",
     "iopub.status.idle": "2025-11-17T18:33:57.182794Z",
     "shell.execute_reply": "2025-11-17T18:33:57.182018Z",
     "shell.execute_reply.started": "2025-11-17T18:33:57.177553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "optimizer = torch.optim.AdamW(new_model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=11, T_mult=2, eta_min=1e-6)\n",
    "loss_fn = TripletHingeLoss(margin=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:33:57.694136Z",
     "iopub.status.busy": "2025-11-17T18:33:57.693465Z",
     "iopub.status.idle": "2025-11-17T18:48:33.991249Z",
     "shell.execute_reply": "2025-11-17T18:48:33.990543Z",
     "shell.execute_reply.started": "2025-11-17T18:33:57.694114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | Train Loss: 29516.8075 | Val Loss: 2278.2054\n",
      "Epoch 10/200 | Train Loss: 13461.9950 | Val Loss: 1615.4733\n",
      "Epoch 15/200 | Train Loss: 49310.7687 | Val Loss: 5233.0007\n",
      "Epoch 20/200 | Train Loss: 38467.8915 | Val Loss: 3283.4297\n",
      "Epoch 25/200 | Train Loss: 21059.5345 | Val Loss: 2996.3296\n",
      "Epoch 30/200 | Train Loss: 10774.1552 | Val Loss: 1339.1135\n",
      "Epoch 35/200 | Train Loss: 30076.2180 | Val Loss: 4397.8713\n",
      "Epoch 40/200 | Train Loss: 51007.3941 | Val Loss: 5791.6785\n",
      "Epoch 45/200 | Train Loss: 54255.5940 | Val Loss: 6331.5757\n",
      "Epoch 50/200 | Train Loss: 42740.8201 | Val Loss: 4225.9597\n",
      "Epoch 55/200 | Train Loss: 32667.7868 | Val Loss: 3405.0907\n",
      "Epoch 60/200 | Train Loss: 19833.6839 | Val Loss: 2747.9568\n",
      "Epoch 65/200 | Train Loss: 12829.5980 | Val Loss: 1424.5031\n",
      "Epoch 70/200 | Train Loss: 8224.7440 | Val Loss: 919.5133\n",
      "Epoch 75/200 | Train Loss: 6503.2148 | Val Loss: 681.5279\n",
      "Epoch 80/200 | Train Loss: 35183.7368 | Val Loss: 3794.5909\n",
      "Epoch 85/200 | Train Loss: 50844.8977 | Val Loss: 4671.1709\n",
      "Epoch 90/200 | Train Loss: 59181.3971 | Val Loss: 7350.5020\n",
      "Epoch 95/200 | Train Loss: 60267.5840 | Val Loss: 8064.5625\n",
      "Epoch 100/200 | Train Loss: 62994.4557 | Val Loss: 9933.9785\n",
      "Epoch 105/200 | Train Loss: 51260.5818 | Val Loss: 7174.6531\n",
      "Epoch 110/200 | Train Loss: 43888.9339 | Val Loss: 5268.8779\n",
      "Epoch 115/200 | Train Loss: 34680.1426 | Val Loss: 4429.2699\n",
      "Epoch 120/200 | Train Loss: 30392.6119 | Val Loss: 5431.8774\n",
      "Epoch 125/200 | Train Loss: 19852.1847 | Val Loss: 2545.2433\n",
      "Epoch 130/200 | Train Loss: 13863.0365 | Val Loss: 2158.9958\n",
      "Epoch 135/200 | Train Loss: 10784.0959 | Val Loss: 1557.4577\n",
      "Epoch 140/200 | Train Loss: 7512.0548 | Val Loss: 1302.1907\n",
      "Epoch 145/200 | Train Loss: 5520.0391 | Val Loss: 677.7133\n",
      "Epoch 150/200 | Train Loss: 4030.4579 | Val Loss: 331.9243\n",
      "Epoch 155/200 | Train Loss: 3132.6305 | Val Loss: 221.4606\n",
      "Epoch 160/200 | Train Loss: 3206.4780 | Val Loss: 165.9859\n",
      "Epoch 165/200 | Train Loss: 2809.9845 | Val Loss: 159.7915\n",
      "Epoch 170/200 | Train Loss: 34256.0967 | Val Loss: 5422.6448\n",
      "Epoch 175/200 | Train Loss: 39034.9744 | Val Loss: 5839.8997\n",
      "Epoch 180/200 | Train Loss: 43543.0438 | Val Loss: 10144.3203\n",
      "Epoch 185/200 | Train Loss: 41684.9607 | Val Loss: 7484.7065\n",
      "Epoch 190/200 | Train Loss: 44608.2233 | Val Loss: 10039.2524\n",
      "Epoch 195/200 | Train Loss: 44346.5790 | Val Loss: 4393.8684\n",
      "Epoch 200/200 | Train Loss: 42234.6177 | Val Loss: 9331.4780\n"
     ]
    }
   ],
   "source": [
    "history = train(new_model, torch.tensor(X), torch.tensor(y), optimizer, loss_fn, scheduler, epochs=200, \n",
    "                batch_size=256, val_split=0.10, device=\"cuda\", patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:02:19.110331Z",
     "iopub.status.busy": "2025-11-17T19:02:19.110055Z",
     "iopub.status.idle": "2025-11-17T19:02:22.650421Z",
     "shell.execute_reply": "2025-11-17T19:02:22.649640Z",
     "shell.execute_reply.started": "2025-11-17T19:02:19.110311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_hat_train = test(new_model, torch.tensor(X), batch_size=128)\n",
    "x_hat_test = test(new_model, torch.tensor(X_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:33:49.617890Z",
     "iopub.status.busy": "2025-11-17T19:33:49.617150Z",
     "iopub.status.idle": "2025-11-17T19:33:49.678530Z",
     "shell.execute_reply": "2025-11-17T19:33:49.677979Z",
     "shell.execute_reply.started": "2025-11-17T19:33:49.617866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.54388e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "reg = Ridge(alpha=0.005).fit(x_hat_train.cpu().numpy(), y)\n",
    "scores = reg.predict(x_hat_test.cpu().numpy())  # Crush outliers\n",
    "# Sub as int(scores) if discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:34:02.316701Z",
     "iopub.status.busy": "2025-11-17T19:34:02.316025Z",
     "iopub.status.idle": "2025-11-17T19:34:02.328107Z",
     "shell.execute_reply": "2025-11-17T19:34:02.327505Z",
     "shell.execute_reply.started": "2025-11-17T19:34:02.316680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3638.000000</td>\n",
       "      <td>3638.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1819.500000</td>\n",
       "      <td>6.082308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1050.344467</td>\n",
       "      <td>0.563132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.999268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>910.250000</td>\n",
       "      <td>5.735916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1819.500000</td>\n",
       "      <td>6.097412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2728.750000</td>\n",
       "      <td>6.464523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3638.000000</td>\n",
       "      <td>8.065002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID        score\n",
       "count  3638.000000  3638.000000\n",
       "mean   1819.500000     6.082308\n",
       "std    1050.344467     0.563132\n",
       "min       1.000000     3.999268\n",
       "25%     910.250000     5.735916\n",
       "50%    1819.500000     6.097412\n",
       "75%    2728.750000     6.464523\n",
       "max    3638.000000     8.065002"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = list(range(1, len(x_hat_test)+1))\n",
    "output = pd.DataFrame(zip(rng, scores-3), columns=[\"ID\", \"score\"])\n",
    "output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14294892,
     "sourceId": 118082,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
